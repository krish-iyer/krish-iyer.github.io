<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>Object Orientation Estimation Algorithm for Visual Feedback Systems | Krishnan&#39;s blog</title>

    <link rel="stylesheet" href="/css/main.css">


      <script src="/js/main.js"></script>


  
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)']]                  
    },
    loader:{
      load: ['ui/safe']
    },
  };
</script>

  

</head>
<body>
  <header>
    
  <nav class="site-nav">
    <ul class="menu-root">
    <li>
      <a href="/">Home</a>
    </li>
    <li>
      <a href="/publications/">Publications</a>
    </li>
    <li>
      <a href="/files/resume.pdf">CV</a>
    </li>
    <li>
      <a href="/about/">About</a>
    </li>
    <li>
      <a href="/contact/">Contact</a>
    </li>
    </ul>
  </nav>


  </header>
  <main>
    
    <h1 class="post-title">Object Orientation Estimation Algorithm for Visual Feedback Systems</h1>

    
    
    <time class="post-date" datetime="2020-05-07T12:32:45&#43;01:00"><em>May 7, 2020</em></time><div class="content-with-toc">
                                  <main class="post-content">
                                    <h2 id="introduction">Introduction</h2>
<p>Let&rsquo;s try to demonstrate a simple statistical approach to find the orientation of the object. There are already existing complex solutions [4] [5] which draw a significant amount of CPU resources, making it impractical to implement in real-time systems. Especially for moving systems, a delay in the orientation could cost a large deviation(error) from the reference. One of the closest approaches is discussed by Dulio Furtado and Fulton T. Ray, Jr [1].</p>
<p>Now, let&rsquo;s try to determine the orientation of the object by considering vector “a”(on the principal axis of the object) and x-axis and orientation will be determined by the relative angle between the two. Vector &ldquo;a&rdquo; is considered to be the shortest side of the triangle. The triangle will be formed by system generated points. These points can be generated by two methods, if it depends on the shape of the object then Vector-Cosine Polygonal Approximation (VCPA) is used and vertex of the object will be considered or else centroids of internal features of the image will be used by Centroid-Vertex Polygon Formation(CVPF). The limitations of this method are that it’s not applicable for highly symmetrical objects and has forced internal features on the object. While the advantage is that it is unidirectional and hence gives the orientation for 360°. While this method uses VCPA and CVPF and both of these methods are iterative methods.</p>
<p>In proposed method the points are identified using median and quartiles which offers a non-iterative approach.</p>
<h2 id="proposed-method">PROPOSED METHOD</h2>
<p>The method is divided into three stages.</p>
<h3 id="feature-detection">Feature Detection</h3>
<p>It’s a crucial phase which determines the boundary region of the feature [3] to be considered for identifying the points. These points will then be considered for forming a line segment along the principal axis of the object. The relative angle between the line segment and one of the axes will determine the orientation of the object. The feature to be detected is not governed by the very shape of the feature although it will have effects on the noise. Objects with rotational symmetry of order 4 will restrict to 90°orientation rather than objects with rotational symmetry of order 2 which will give 180°orientation.</p>
<h3 id="identification-of-points">Identification of points</h3>
<p>The median and quartiles as shown in “Fig. 1” makes the orientation independent of the shape and position of the feature in the camera frame as median serves as the middle value of the sorted set and quartile being a second point in the feature gives the slope relative to the camera frame. This statistical approach ensures that points will always lie in the boundary values of the feature. Hence a significant amount of error and noise is being avoided and has a robust response.</p>
<h3 id="determining-orientation">Determining orientation</h3>
<p>After points have been identified, orientation can easily be calculated by</p>
\[ tan^-1 ( \frac{x}{y} ) \]<p>. The magnitude of the angle will be calculated by the slope as shown in “Fig. 2” where as the direction(anticlockwise or clockwise)(shown in “Fig. 4”) will be given by the difference of y value respect to index value of x quartile and y median as shown in “Fig. 3”.</p>
<div style="display: flex; justify-content: space-around; align-items: flex-end;">
  <figure style="flex: 1; margin: 0 10px;">
    <img src="/files/images/object_est_fig1.png" alt="Image 1 Description" style="width: 100%;">
    <figcaption style="margin-top: 10px; word-wrap: break-word;">Fig. 1. Identification of Points</figcaption>
  </figure>
  <figure style="flex: 1; margin: 0 10px;">
    <img src="/files/images/object_est_fig2.png" alt="Image 2 Description" style="width: 100%;">
    <figcaption style="margin-top: 10px; word-wrap: break-word;">Fig. 2. Orientation Angle vs Time (ideal) for π/2 to − π/2 orientation</figcaption>
  </figure>
  <figure style="flex: 1; margin: 0 10px;">
    <img src="/files/images/object_est_fig3.png" alt="Image 3 Description" style="width: 100%;">
    <figcaption style="margin-top: 10px; word-wrap: break-word;">Fig. 3. Difference of y values vs Time (y value with respect to index value of x quartile and y median) (ideal) for π/2 to − π/2 orientation</figcaption>
  </figure>
</div>
<h2 id="implementation">IMPLEMENTATION</h2>
<p>We tested this on a testbed consisting of a camera [7] [8] fixed orthogonal to the object feature. Feature detection is done based on a unique colour on the object in the camera frame and the feature is a blue rectangular shaped strip. The algorithm is implemented in Python programming language using OpenCV library and been implemented in following steps.</p>
<h3 id="feature-detection-1">Feature Detection</h3>
<ul>
<li>Convert the frame into HSV colour model(HSV separates image intensity(environmental parameters) from actual colour code)</li>
<li>Mask with the required colour code(extraction of the feature based on color).</li>
<li>Morphological transformation(dilation + erosion) to eliminate the noise.</li>
</ul>
<h3 id="identification-of-points-1">Identification of points</h3>
<ul>
<li>Separate row and column of detected feature</li>
<li>Find row(\(x_m\)) and column(\(y_m\)) median</li>
<li>Find a quartile with 25% in row(\(x_q\)) and column(\(y_q\)) array
from midpoint.</li>
<li>Find the index of the above value in row array</li>
<li>For the same index find the value in column index(\(y_{\text{ind}}\))</li>
</ul>
<h3 id="orientation-estimation">Orientation estimation</h3>
<ul>
<li>Find the slope of the line segment joining \(x_m\) , \(y_m\) and \(x_q\),\(y_q\) by \(tan^−1(\frac{x_m − x_q }{y_m − y_q})\)</li>
<li>For the direction(anticlockwise or clockwise) will be
given by the difference of y value respect to index value of x quartile and y median \(y_m − y_{\text{ind}}\)</li>
</ul>
<div style="display: flex; justify-content: space-around; align-items: flex-end;">
	<figure style="flex: 1; margin: 0 10px;">
		<img src="/files/images/object_est_fig4.png" alt="Image 1 Description" style="width: 100%;">
		<figcaption style="margin-top: 10px; word-wrap: break-word;">Fig. 4. Direction estimated based on the difference of y points (-1 represents anticlockwise, 1 represents clockwise direction and 0 represents reference interval) (ideal) for π/2 to − π/2 orientation</figcaption>
	</figure>
	<figure style="flex: 1; margin: 0 10px;">
		<img src="/files/images/object_est_fig5.png" alt="Image 2 Description" style="width: 100%;">
		<figcaption style="margin-top: 10px; word-wrap: break-word;">Fig. 5. Orientation Angle vs Time (practical) for π/2 to − π/2 orientation</figcaption>
	</figure>
	<figure style="flex: 1; margin: 0 10px;">
		<img src="/files/images/object_est_fig6.png" alt="Image 3 Description" style="width: 100%;">
		<figcaption style="margin-top: 10px; word-wrap: break-word;">Fig. 6. Difference of y values (y value with respect to index value of x quartile and y median) (practical) for π/2 to − π/2 orientation</figcaption>
	</figure>
</div>
<p>The orientation estimation and direction graphs are shown in “Fig. 5” and “Fig. 8” respectively. The noise due to the difference as shown in “Fig. 6” operation in estimation of direction is filtered with a low pass filter(moving average filter) as shown in “Fig. 7”</p>
<div style="display: flex; justify-content: space-between;">
  <figure style="flex: 1; margin: 0 10px;">
    <img src="/files/images/object_est_fig6.png" alt="Image 1 Description" style="max-width: 100%; height: auto;">
    <figcaption style="margin-top: 10px;">Fig. 7. Applied Filtered on Difference of y points</figcaption>
  </figure>
  <figure style="flex: 1; margin: 0 10px;">
    <img src="/files/images/object_est_fig7.png" alt="Image 2 Description" style="max-width: 100%; height: auto;">
    <figcaption style="margin-top: 10px;">Fig. 8. Direction estimated based on the difference of y points (-1 represents anticlockwise, 1 represents clockwise direction and 0 represents reference interval) (practical) for π/2 to − π/2 orientation</figcaption>
  </figure>
</div>
<h2 id="conclusion">CONCLUSION</h2>
<p>Hence a simple statistical method of estimating orientation [6] was implemented for real-time applications. This method is very well applicable to images as it doesn’t depend on successive frames to determine the orientation. One of the applications could be for the implementation of yaw control of drone [2] in an indoor environment keeping camera on the feedback for the control.</p>
<h2 id="references">REFERENCES</h2>
<ol>
<li>Dulio Furtado, Fulton T. Ray, A Rule Based 2-D Vision System to Determine Part Orientation, IFAC Proceedings Volumes, Volume 25, Issue 28, 1992, Pages 287-293.</li>
<li>N. Shijith and M. M. Dharmana, ”Sonar based terrain estimation &amp; automatic landing of swarm quadrotors,” 2017 International Conference on Circuit ,Power and Computing Technologies (ICCPCT), Kollam, 2017, pp. 1-4.</li>
<li>A. Alexander and M. M. Dharmana, ”Object detection algorithm for segregating similar coloured objects and database formation,” 2017 International Conference on Circuit ,Power and Computing Technologies (ICCPCT), Kollam, 2017, pp. 1-5.</li>
<li>C. Tsai, C. Wong, T. Liu and A. Tsao, ”A novel image-based object orientation estimation algorithm for robotic manipulator applications,” 2012 International Symposium on Intelligent Signal Processing and Communications Systems, Taipei, 2012, pp. 280-284.</li>
<li>V. Sintunata and T. Aoki, ”Object orientation estimation for high speed 3D object retrieval system,” 2016 4th International Conference on Information and Communication Technology (ICoICT), Bandung, 2016, pp. 1-6.</li>
<li>C. Ks and H. Nicolas, ”Rough compressed domain camera pose estima- tion through object motion,” 2009 16th IEEE International Conference on Image Processing (ICIP), Cairo, 2009, pp. 3481-3484.</li>
<li>TaryudiandM.Wang,”3Dobjectposeestimationusingstereovisionfor object manipulation system,” 2017 International Conference on Applied System Innovation (ICASI), Sapporo, 2017, pp. 1532-1535.</li>
<li>J. Ku, A. D. Pon, S. Walsh and S. L. Waslander, ”Improving 3D Object Detection for Pedestrians with Virtual Multi-View Synthesis Orientation Estimation,” 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019, pp. 3459-3466.</li>
</ol>

                                  </main>
                                  <aside class="post-toc">
                                    <h3>Table of Contents</h3>
                                    <nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#proposed-method">PROPOSED METHOD</a>
      <ul>
        <li><a href="#feature-detection">Feature Detection</a></li>
        <li><a href="#identification-of-points">Identification of points</a></li>
        <li><a href="#determining-orientation">Determining orientation</a></li>
      </ul>
    </li>
    <li><a href="#implementation">IMPLEMENTATION</a>
      <ul>
        <li><a href="#feature-detection-1">Feature Detection</a></li>
        <li><a href="#identification-of-points-1">Identification of points</a></li>
        <li><a href="#orientation-estimation">Orientation estimation</a></li>
      </ul>
    </li>
    <li><a href="#conclusion">CONCLUSION</a></li>
    <li><a href="#references">REFERENCES</a></li>
  </ul>
</nav>
                                  </aside>
                                </div>


  </main>
  <footer>
    © 2025 Krishnan Iyer.
Original content licensed under
<a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>, except where otherwise noted.

  </footer>
</body>
</html>
